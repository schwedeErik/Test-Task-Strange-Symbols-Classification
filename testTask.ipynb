{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strange Symbols Classification Task\n",
    "\n",
    "This notebook tackles the **Strange Symbols Classification** test task.\n",
    "\n",
    "## Dataset Analysis\n",
    "The first step involved analyzing the provided dataset to gain a better understanding of the data. Here’s what was done:  \n",
    "- Checked the **dimensionality** of the dataset.  \n",
    "- Visualized the provided grayscale images to understand their structure.\n",
    "\n",
    "### Key Findings\n",
    "- The dataset contains **15,000 grayscale images**, each of size **28x28 pixels**.  \n",
    "- Each image is assigned an **integer label** ranging from **0 to 14**, making this a **15-class classification problem**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image count: 15000\n",
      "Image Shape: torch.Size([28, 28])\n",
      "Lable type: <class 'int'>\n",
      "Distinct labels:\n",
      " {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14}\n",
      "\n",
      "Image Examples:\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 8\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "training_data = torch.load('strange_symbols/training_data.pt')\n",
    "training_labels = torch.load('strange_symbols/training_labels.pt')\n",
    "\n",
    "print(f'Image count: {len(training_data)}')\n",
    "print(f'Image Shape: {training_data[0].shape}')\n",
    "print(f'Lable type: {type(training_labels[0])}')\n",
    "print(f'Distinct labels:\\n {set(training_labels)}\\n')\n",
    "\n",
    "print('Image Examples:\\n')\n",
    "for index in range(2):\n",
    "    plt.imshow(training_data[index].numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    print(f'Label: {training_labels[index]}')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `CustomTensorDataset` Class\n",
    "\n",
    "The `CustomTensorDataset` class is designed to format the input data, making it compatible with the **`Conv2d` layers** in PyTorch.  \n",
    "\n",
    "### Key Features\n",
    "- **Data Normalization**: Converts 8-bit integer values into **float values** ranging between **0 and 1**.  \n",
    "- **Dimensionality Adjustment**: Adds an additional dimension to the data, as required by the **`Conv2d` layers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "# Custom Dataset class to handle the .pt files\n",
    "class CustomTensorDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data.float() / 255.0\n",
    "        self.data = self.data.unsqueeze(1)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        if self.labels is not None:\n",
    "            y = self.labels[idx]\n",
    "            return x, y\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "\n",
    "# Instantiate datasets and dataloaders\n",
    "train_dataset = CustomTensorDataset(training_data, training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `SimpleCNN` Class\n",
    "\n",
    "The `SimpleCNN` class implements the neural network for this task.  \n",
    "\n",
    "### Why a CNN?\n",
    "Since we are working with image data, a **Convolutional Neural Network (CNN)** is a suitable choice. CNNs are effective in:  \n",
    "- Preserving **spatial information** in images.  \n",
    "- Extracting **relevant features** essential for classification.\n",
    "\n",
    "### Network Design\n",
    "- Due to the relatively small size of the dataset, the network includes **two convolutional layers**.  \n",
    "- During the forward pass:\n",
    "  - **ReLU activation function** is applied after each convolution.  \n",
    "  - **Max pooling** is used to reduce the size of the images and retain key features.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a simple CNN for image classification\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 128) \n",
    "        self.fc2 = nn.Linear(128, 15)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training: 5-Fold Cross-Validation\n",
    "\n",
    "To train the model, **5-fold cross-validation** is employed using the `StratifiedKFold` class from the **scikit-learn** library. This approach is particularly beneficial for classification tasks as it helps avoid biases by maintaining the proportion of classes in each fold.\n",
    "\n",
    "### Training Process\n",
    "- The model is trained **individually** for each split of the validation and training sets.  \n",
    "- **Batch size**: 32  \n",
    "- **Optimizer**: Adam (built into PyTorch).  \n",
    "- **Training progress**: Visualized using the `tqdm` library.\n",
    "\n",
    "### Epochs and Early Stopping\n",
    "- The model is trained for **5 epochs** (a hyperparameter that can be adjusted as needed).  \n",
    "- Optionally, an **early stopping** mechanism can be implemented to monitor losses and terminate training if losses rise beyond a certain threshold, helping to prevent overfitting.\n",
    "\n",
    "### Code Organization\n",
    "The training process is divided into three separate functions to:\n",
    "- Improve code readability.  \n",
    "- Avoid the complexity of nested loops.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def calc_val_loss(model, val_loader, criterion):\n",
    "    '''Calculate the validation loss for the specified model validation data and loss function'''\n",
    "    val_loss = 0\n",
    "    # Loop thru the validation data while not computing the gradient\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            output = model(data)\n",
    "            val_loss += criterion(output, target).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    return val_loss\n",
    "\n",
    "def calc_val_accuracy(model, val_loader, criterion):\n",
    "    '''Calculate the validation loss for the specified model validation data and loss function'''\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0\n",
    "    # Loop thru the validation data while not computing the gradient\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            output = model(data)\n",
    "            _,predicted = torch.max(output,1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            val_loss += criterion(output, target).item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def forward_backward_propagation(model, train_loader, optimizer, criterion):\n",
    "    '''Perfom forward and backward propagation'''\n",
    "    for data, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs, criterion, optimizer):\n",
    "    '''Perfom training loop over the specified parameters'''\n",
    "    for epoch in tqdm(range(epochs)):  # Adjust the number of epochs as needed\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        forward_backward_propagation(model, train_loader, optimizer, criterion)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        # Calculate the loss with the validation set\n",
    "        val_accuracy = calc_val_accuracy(model, val_loader, criterion)\n",
    "        print(f'Validation accuracy in  epoch {epoch+1}: {val_accuracy:.4f} %')\n",
    "        \n",
    "    return val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Training Implementation\n",
    "\n",
    "Below is the **final training implementation**, which integrates the previously defined functions. These functions streamline the process and ensure the training pipeline is clean and organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:03<00:12,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy in  epoch 1: 85.6667 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:06<00:09,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy in  epoch 2: 89.7667 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:09<00:06,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy in  epoch 3: 89.5000 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:13<00:03,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy in  epoch 4: 89.7333 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:16<00:00,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy in  epoch 5: 91.5000 %\n",
      "Fold 1 completed with Validation accuracy: 91.5000 %\n",
      "\n",
      "\n",
      "Fold 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:03<00:12,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy in  epoch 1: 85.9667 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:06<00:09,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy in  epoch 2: 89.0333 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:09<00:06,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy in  epoch 3: 89.9000 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:13<00:03,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy in  epoch 4: 90.5333 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:16<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy in  epoch 5: 90.9667 %\n",
      "Fold 2 completed with Validation accuracy: 90.9667 %\n",
      "\n",
      "\n",
      "Fold 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:03<00:12,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy in  epoch 1: 85.1333 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:06<00:09,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy in  epoch 2: 88.8667 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:09<00:06,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy in  epoch 3: 90.3333 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:13<00:03,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy in  epoch 4: 89.4000 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:16<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy in  epoch 5: 91.1667 %\n",
      "Fold 3 completed with Validation accuracy: 91.1667 %\n",
      "\n",
      "\n",
      "Fold 4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:03<00:12,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy in  epoch 1: 86.0667 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:06<00:09,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy in  epoch 2: 88.5333 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:09<00:06,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy in  epoch 3: 89.6667 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:13<00:03,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy in  epoch 4: 90.4000 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:16<00:00,  3.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy in  epoch 5: 91.3000 %\n",
      "Fold 4 completed with Validation accuracy: 91.3000 %\n",
      "\n",
      "\n",
      "Fold 5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:03<00:12,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy in  epoch 1: 86.1000 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:06<00:09,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy in  epoch 2: 89.5333 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:09<00:06,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy in  epoch 3: 90.4667 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:13<00:03,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy in  epoch 4: 91.2667 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:16<00:00,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy in  epoch 5: 91.8000 %\n",
      "Fold 5 completed with Validation accuracy: 91.8000 %\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "folds = 5\n",
    "kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=11)\n",
    "val_accuracies = []\n",
    "#Specify loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 5\n",
    "\n",
    "#Loop thrue the k folds of f fold cross validation \n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset,training_labels)):\n",
    "    print(f'\\nFold {fold + 1}:')\n",
    "\n",
    "    train_loader = DataLoader(Subset(train_dataset, train_idx), batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(Subset(train_dataset, val_idx), batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize the model and optimizer\n",
    "    model = SimpleCNN()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    # Training loop runs for every epoch\n",
    "    val_accuracy = train_model(model, train_loader, val_loader, epochs, criterion, optimizer)\n",
    "    val_accuracies.append(val_accuracy)    \n",
    "    print(f\"Fold {fold+1} completed with Validation accuracy: {val_accuracy:.4f} %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation accuracy across 5 folds: 91.3467 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "average_val_loss = np.mean(val_accuracies)\n",
    "print(f\"Average Validation accuracy across {5} folds: {average_val_loss:.4f} %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
